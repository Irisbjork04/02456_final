{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0d4e0a",
   "metadata": {},
   "source": [
    "## Implementation of the VAE+ and trainning procedure\n",
    "The implementation inspired by: https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/7_Unsupervised/7.2-EXE-variational-autoencoder.ipynb and https://github.com/tueimage/cytoVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d8cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor, sigmoid\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Bernoulli, Normal\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e04b7d",
   "metadata": {},
   "source": [
    "### Evaluation and plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b86ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None,\n",
    "               device='cpu'):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, (test_batch, test_target) in enumerate(test_loader):\n",
    "        test_batch = test_batch.to(device)\n",
    "        \n",
    "        #loss_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss_t, diagnostics, outputs = vi(model_best, test_batch)\n",
    "        loss = loss + loss_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}')\n",
    "\n",
    "    return loss\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = next(iter(test_loader))[0].detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.transpose(x[i].reshape((3, 68, 68)), (1, 2, 0))\n",
    "        ax.imshow(plottable_image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    x = next(iter(data_loader))[0].detach().numpy()\n",
    "\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    \n",
    "    px = model_best.sample_from_prior(batch_size=num_x * num_y)['px']\n",
    "    x_samples = px.sample()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x_samples[i], (3, 68, 68)).permute(1, 2, 0)\n",
    "        ax.imshow(plottable_image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_curve(name, nll_val):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51a3f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def plot_samples(ax, x):\n",
    "    x = x.to('cpu')\n",
    "    nrow = int(np.sqrt(x.size(0)))\n",
    "    x_grid = make_grid(x.view(-1, 3, 68, 68), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid)\n",
    "    ax.axis('off')\n",
    "\n",
    "def plot_interpolations(ax, vae):\n",
    "    device = next(iter(vae.parameters())).device\n",
    "    nrow = 10\n",
    "    nsteps = 10\n",
    "    prior_params = vae.prior_params.expand(2 * nrow, *vae.prior_params.shape[-1:])\n",
    "    mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "    pz = Normal(mu, log_sigma.exp())\n",
    "    z = pz.sample().view(nrow, 2, -1)\n",
    "    t = torch.linspace(0, 1, 10, device=device)\n",
    "    zs = t[None, :, None] * z[:, 0, None, :] + (1 - t[None, :, None]) * z[:, 1, None, :]\n",
    "    px = vae.observation_model(zs.view(nrow * nsteps, -1))\n",
    "    x = px.sample()\n",
    "    x = x.to('cpu')\n",
    "    x_grid = make_grid(x.view(-1, 3, 68, 68), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def plot_grid(ax, vae):\n",
    "    device = next(iter(vae.parameters())).device\n",
    "    nrow = 10\n",
    "    xv, yv = torch.meshgrid([torch.linspace(-3, 3, 10), torch.linspace(-3, 3, 10)])\n",
    "    zs = torch.cat([xv[:, :, None], yv[:, :, None]], -1)\n",
    "    zs = zs.to(device)\n",
    "    px = vae.observation_model(zs.view(nrow * nrow, 2))\n",
    "    x = px.sample()\n",
    "    x = x.to('cpu')\n",
    "    x_grid = make_grid(x.view(-1, 3, 68, 68), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def plot_2d_latents(ax, qz, z, y):\n",
    "    z = z.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    scale_factor = 2\n",
    "    batch_size = z.shape[0]\n",
    "    palette = sns.color_palette()\n",
    "    colors = [palette[l] for l in y]\n",
    "\n",
    "    # plot prior\n",
    "    prior = plt.Circle((0, 0), scale_factor, color='gray', fill=True, alpha=0.1)\n",
    "    ax.add_artist(prior)\n",
    "\n",
    "    # plot data points\n",
    "    mus, sigmas = qz.mu.to('cpu'), qz.sigma.to('cpu')\n",
    "    mus = [mus[i].numpy().tolist() for i in range(batch_size)]\n",
    "    sigmas = [sigmas[i].numpy().tolist() for i in range(batch_size)]\n",
    "\n",
    "    posteriors = [\n",
    "        plt.matplotlib.patches.Ellipse(mus[i], *(scale_factor * s for s in sigmas[i]), color=colors[i], fill=False,\n",
    "                                       alpha=0.3) for i in range(batch_size)]\n",
    "    for p in posteriors:\n",
    "        ax.add_artist(p)\n",
    "\n",
    "    ax.scatter(z[:, 0], z[:, 1], color=colors)\n",
    "\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "\n",
    "def plot_latents(ax, z, y):\n",
    "    z = z.to('cpu')\n",
    "    palette = sns.color_palette()\n",
    "    colors = [palette[l] for l in y]\n",
    "    z = TSNE(n_components=2).fit_transform(z)\n",
    "    ax.scatter(z[:, 0], z[:, 1], color=colors)\n",
    "\n",
    "\n",
    "def make_vae_plots(vae, x, y, outputs, training_data, validation_data, \n",
    "                   tmp_img=\"tmp_vae_out.png\", save_img=\"vae_out.png\", figsize=(18, 18), save=False):\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=figsize, squeeze=False)\n",
    "\n",
    "    # plot the observation\n",
    "    axes[0, 0].set_title(r'Observation $\\mathbf{x}$')\n",
    "    print(x.shape)\n",
    "    fig = plot_samples(axes[0, 0], x) \n",
    "\n",
    "    # plot the latent samples\n",
    "    try:\n",
    "        z = outputs['z']\n",
    "        if z.shape[1] == 2:\n",
    "            axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
    "            qz = outputs['qz']\n",
    "            plot_2d_latents(axes[0, 1], qz, z, y)\n",
    "        else:\n",
    "            axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ (t-SNE)')\n",
    "            plot_latents(axes[0, 1], z, y)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate the plot of the latent sanples because of exception\")\n",
    "        print(e)\n",
    "\n",
    "    # plot posterior samples\n",
    "    axes[0, 2].set_title(\n",
    "        r'Reconstruction $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
    "    px = outputs['px']\n",
    "    x_sample = px.sample().to('cpu')\n",
    "    print(x_sample.shape)\n",
    "    plot_samples(axes[0, 2], x_sample)\n",
    "\n",
    "    # plot ELBO\n",
    "    ax = axes[1, 0]\n",
    "    ax.set_title(r'ELBO: $\\mathcal{L} ( \\mathbf{x} )$')\n",
    "    ax.plot(training_data['elbo'], label='Training')\n",
    "    ax.plot(validation_data['elbo'], label='Validation')\n",
    "    ax.legend()\n",
    "\n",
    "    # plot KL\n",
    "    ax = axes[1, 1]\n",
    "    ax.set_title(r'$\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$')\n",
    "    ax.plot(training_data['kl'], label='Training')\n",
    "    ax.plot(validation_data['kl'], label='Validation')\n",
    "    ax.legend()\n",
    "\n",
    "    # plot NLL\n",
    "    ax = axes[1, 2]\n",
    "    ax.set_title(r'$\\log p_\\theta(\\mathbf{x} | \\mathbf{z})$')\n",
    "    ax.plot(training_data['log_px'], label='Training')\n",
    "    ax.plot(validation_data['log_px'], label='Validation')\n",
    "    ax.legend()\n",
    "\n",
    "    # plot prior samples\n",
    "    axes[2, 0].set_title(r'Samples $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim p(\\mathbf{z})$')\n",
    "    px = vae.sample_from_prior(batch_size=x.size(0))['px']\n",
    "    x_samples = px.sample()\n",
    "    print(x_samples.shape)\n",
    "    plot_samples(axes[2, 0], x_samples)\n",
    "\n",
    "    # plot interpolations samples\n",
    "    axes[2, 1].set_title(\n",
    "        r'Latent Interpolations: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | t \\cdot \\mathbf{z}_1 + (1-t) \\cdot \\mathbf{z}_2), \\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z}), t=0 \\dots 1$')\n",
    "    plot_interpolations(axes[2, 1], vae)\n",
    "\n",
    "    # plot samples (sampling from a grid instead of the prior)\n",
    "    if vae.latent_features == 2:\n",
    "        axes[2, 2].set_title(\n",
    "            r'Samples: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim \\operatorname{grid}(-3:3, -3:3)$')\n",
    "        px = vae.sample_from_prior(batch_size=x.size(0))['px']\n",
    "        x_samples = px.sample()\n",
    "        plot_grid(axes[2, 2], vae)\n",
    "        \n",
    "    if save:\n",
    "        plt.savefig(save_img)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        # display\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(tmp_img)\n",
    "        plt.close(fig)    \n",
    "        display(Image(filename=tmp_img))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e52cd6",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contain 68x68 images of single cells treated with different compounds. For each of the utilized compounds there is an associated mechanism of action (moa), which describes how the compound it affecting the cell. There are 12 different classes of moa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb791c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.read_csv wiht pyarrow took 2.4474189281463623 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "metadata = pd.read_csv('/Users/mikkelrasmussen/OneDrive - Danmarks Tekniske Universitet'\n",
    "                       '/DTU/Bioinformatics_and_Systems_Biology/1_semester/Deep learning (02456)/'\n",
    "                       'deep_learning_project/metadata.csv', engine=\"pyarrow\")\n",
    "print(\"pd.read_csv wiht pyarrow took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8a6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_value = 16000\n",
    "\n",
    "# DMSO category\n",
    "DMSO_indx = metadata.index[metadata['moa'] == 'DMSO']\n",
    "DMSO_drop_indices = np.random.choice(DMSO_indx, size=len(DMSO_indx) - downsample_value, replace=False)\n",
    "\n",
    "# Microtubule stabilizers\n",
    "#micro_indx = metadata.index[metadata['moa'] == 'Microtubule stabilizers']\n",
    "#micro_drop_indices = np.random.choice(micro_indx, size=len(micro_indx) - downsample_value, replace=False)\n",
    "\n",
    "#print(len(np.intersect1d(DMSO_drop_indices, micro_drop_indices)))\n",
    "#all_drop_indices = np.concatenate((DMSO_drop_indices, micro_drop_indices))\n",
    "all_drop_indices = DMSO_drop_indices\n",
    "\n",
    "metadata_subsampled = metadata.drop(all_drop_indices, axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9a8e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moa</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "      <td>89157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aurora kinase inhibitors</td>\n",
       "      <td>16810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DNA damage</td>\n",
       "      <td>16582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMSO</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microtubule destabilizers</td>\n",
       "      <td>15178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Epithelial</td>\n",
       "      <td>14955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eg5 inhibitors</td>\n",
       "      <td>12525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kinase inhibitors</td>\n",
       "      <td>11622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Protein synthesis</td>\n",
       "      <td>9715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>7491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Protein degradation</td>\n",
       "      <td>6589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DNA replication</td>\n",
       "      <td>5976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholesterol-lowering</td>\n",
       "      <td>5436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          moa  counts\n",
       "10    Microtubule stabilizers   89157\n",
       "1    Aurora kinase inhibitors   16810\n",
       "4                  DNA damage   16582\n",
       "3                        DMSO   16000\n",
       "9   Microtubule destabilizers   15178\n",
       "7                  Epithelial   14955\n",
       "6              Eg5 inhibitors   12525\n",
       "8           Kinase inhibitors   11622\n",
       "12          Protein synthesis    9715\n",
       "0            Actin disruptors    7491\n",
       "11        Protein degradation    6589\n",
       "5             DNA replication    5976\n",
       "2        Cholesterol-lowering    5436"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_subsampled.groupby(\"moa\").size().reset_index(name='counts').sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f24eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from class name to class index\n",
    "classes = {index: name for name, index in enumerate(metadata[\"moa\"].unique())}\n",
    "classes_inv = {v: k for k, v in classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, images_folder, class_map, \n",
    "                 mode='train', transform = None):\n",
    "        self.df = annotation_file\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.class2index = class_map\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.loc[index, \"Single_Cell_Image_Name\"]\n",
    "        label = self.class2index[self.df.loc[index, \"moa\"]]\n",
    "        subfolder = re.search(\"(.*)_\", filename).group(1)\n",
    "        image = np.load(os.path.join(self.images_folder, subfolder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.astype(np.float32))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f8486",
   "metadata": {},
   "source": [
    "### Reparameterization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d90830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        z = self.mu + self.sigma * self.sample_epsilon()\n",
    "        return z\n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return -((z-self.mu)**2) / (2*self.sigma**2) - self.sigma.log() - math.log(math.sqrt(2 * math.pi))\n",
    "        #return torch.distributions.Normal(self.mu, self.sigma).log_prob(z) #  alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f0883bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussianWithSigmoid(ReparameterizedDiagonalGaussian):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        super().__init__(mu, log_sigma)\n",
    "    \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        z = self.mu + self.sigma * self.sample_epsilon()\n",
    "        return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "659861a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1824\n",
      "228\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The loaders perform the actual work\n",
    "images_folder = \"/Users/mikkelrasmussen/mnt/deep_learning_project/data/singh_cp_pipeline_singlecell_images\"\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "     transforms.Lambda(lambda x: x/x.max())]\n",
    ")\n",
    "\n",
    "train_set = SingleCellDataset(images_folder=images_folder, \n",
    "                              annotation_file=metadata_subsampled, \n",
    "                              transform=train_transforms,\n",
    "                              class_map=classes)\n",
    "\n",
    "# Define the size of the train, validation and test datasets\n",
    "data_prct = 0.01\n",
    "train_prct = 0.8\n",
    "\n",
    "data_amount = int(len(metadata_subsampled) * data_prct)\n",
    "train_size = int(train_prct * data_amount)\n",
    "val_size = (data_amount - train_size) // 2\n",
    "test_size = (data_amount - train_size) // 2\n",
    "\n",
    "indicies = torch.randperm(len(metadata_subsampled))\n",
    "train_indices = indicies[:train_size]\n",
    "val_indicies = indicies[train_size:train_size+val_size]\n",
    "test_indicies = indicies[train_size+val_size:train_size+val_size+test_size]\n",
    "\n",
    "# Checking there are not overlapping incdicies\n",
    "#print(sum(np.isin(train_indices.numpy() , [val_indicies.numpy(), test_indicies.numpy()])))\n",
    "#print(sum(np.isin(val_indicies.numpy() , [train_indices.numpy(), test_indicies.numpy()])))\n",
    "#print(sum(np.isin(test_indicies.numpy() , [train_indices.numpy(), val_indicies.numpy()])))\n",
    "\n",
    "training_set = torch.utils.data.Subset(train_set, train_indices.tolist())\n",
    "val_set = torch.utils.data.Subset(train_set, val_indicies.tolist())\n",
    "testing_set = torch.utils.data.Subset(train_set, test_indicies.tolist())\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(training_loader.dataset))\n",
    "print(len(val_loader.dataset))\n",
    "print(len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1bdcdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b236c",
   "metadata": {},
   "source": [
    "### Variational autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e85c8507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=13872, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=27744, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nbUnits = 32\n",
    "latent_features = 256\n",
    "\n",
    "class PrintSize(nn.Module):\n",
    "    \"\"\"Utility module to print current shape of a Tensor in Sequential, only at the first pass.\"\"\"\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.first:\n",
    "            print(f\"Size: {x.size()}\")\n",
    "            self.first = False\n",
    "        return x\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=latent_features):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=128, out_features=2*latent_features), # <- note the 2*latent_features\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=2*self.observation_features)\n",
    "        )\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        #h_x = h_x.squeeze()\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        #px_output = self.decoder(z)\n",
    "        #px_output = px_output.view(-1, *self.input_shape) # reshape the output\n",
    "        \n",
    "        #return Bernoulli(logits=px_output, validate_args=False)\n",
    "        \n",
    "        px_output = self.decoder(z)\n",
    "        mu, log_sigma = px_output.chunk(2, dim=1)\n",
    "        \n",
    "        return ReparameterizedDiagonalGaussianWithSigmoid(mu, log_sigma) # Sample from the Normal distribution\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        #x = x.view(x.size(0), -1)\n",
    "\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample() # qz.mu for test\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "206275af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (activation): ReLU()\n",
      "  (batchnorm1_2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2_2d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3_2d): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_first): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv_mid): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv_last): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv_out): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (max_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape:torch.Size, conv_channels:int, \n",
    "                 kernel_size:int, stride:int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.conv_channels = conv_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # Defining activation function\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Define batchnorms\n",
    "        self.batchnorm1_2d = nn.BatchNorm2d(self.conv_channels)\n",
    "        self.batchnorm2_2d = nn.BatchNorm2d(64)\n",
    "        self.batchnorm3_2d = nn.BatchNorm2d(1)\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.conv_first = nn.Conv2d(in_channels=self.input_shape[1],\n",
    "                             out_channels=self.conv_channels,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             stride=self.stride)\n",
    "        \n",
    "        self.conv_mid = nn.Conv2d(in_channels=self.conv_channels,\n",
    "                               out_channels=self.conv_channels,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               stride=self.stride)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(in_channels=conv_channels,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               stride=self.stride)\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(in_channels=64,\n",
    "                               out_channels=1,\n",
    "                               kernel_size=1,\n",
    "                               stride=self.stride)\n",
    "        \n",
    "        # Max-pooling layer\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2, 2))\n",
    "        \n",
    "\n",
    "    def forward(self, x_img):\n",
    "        \n",
    "        ## Convolutional layers ##\n",
    "        \n",
    "        # Layer 0 [hw_in=68, hw_out=64>32]: INPUT LAYER\n",
    "        x_img = self.conv_first(x_img) # W2=(W1−K+2P)/S]+1=[(68 - 5 + 2*0)/1]+1 = 64\n",
    "        x_img = self.max_pool(x_img) # W2=(W1−F)/S+1]=[(64 - 2)/2] + 1 = 32\n",
    "        x_img = self.activation(x_img)\n",
    "        x_img = self.batchnorm1_2d(x_img)\n",
    "        x_img_1 = x_img\n",
    "        \n",
    "        # Layer 1 [hw_in=32, hw_out=28>14]\n",
    "        x_img = self.conv_mid(x_img) # W2=(W1−K+2P)/S]+1=[(32 - 5 + 2*0)/1]+1 = 28\n",
    "        x_img = self.max_pool(x_img) # W2=(W1−F)/S+1]=[(28 - 2)/2] + 1 = 14\n",
    "        x_img = self.activation(x_img)\n",
    "        x_img = self.batchnorm1_2d(x_img)\n",
    "        x_img_2 = x_img\n",
    "        \n",
    "        # Layer 2 [hw_in=14, hw_out=10>5]\n",
    "        x_img = self.conv_mid(x_img) # W2=(W1−K+2P)/S]+1=[(14 - 5 + 2*0)/1]+1 = 10\n",
    "        x_img = self.max_pool(x_img) # W2=(W1−F)/S+1]=[(10 - 2)/2] + 1 = 5\n",
    "        x_img = self.activation(x_img)\n",
    "        x_img = self.batchnorm1_2d(x_img)\n",
    "        x_img_3 = x_img\n",
    "        \n",
    "        # Layer 3 [hw_in=5, hw_out=1]: \n",
    "        x_img = self.conv_last(x_img) # W2=(W1−K+2P)/S]+1=[(5 - 5 + 2*0)/1]+1 = 1\n",
    "        x_img = self.activation(x_img)\n",
    "        x_img = self.batchnorm2_2d(x_img)\n",
    "        x_img_4 = x_img\n",
    "        \n",
    "        # Layer 4 [hw_in=1, hw_out=1]:\n",
    "        x_img = self.conv_out(x_img) # W2=(W1−K+2P)/S]+1=[(1 - 1 + 2*0)/1]+1 = 1\n",
    "        x_img = self.batchnorm3_2d(x_img)\n",
    "        x_img_5 = x_img\n",
    "        \n",
    "        intermediate_rep = [x_img_1, x_img_2, x_img_3, x_img_4, x_img_5]\n",
    "        \n",
    "        output = x_img\n",
    "        \n",
    "        return output, intermediate_rep\n",
    "\n",
    "discrim_test = Discriminator(images.reshape((128, 3, 68, 68)).shape, conv_channels=32, \n",
    "                 kernel_size=5, stride=1)\n",
    "print(discrim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b856e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output | shape: [128, 1, 1, 1]\n",
      "x_img_1 | shape: [128, 32, 32, 32]\n",
      "x_img_2 | shape: [128, 32, 14, 14]\n",
      "x_img_3 | shape: [128, 32, 5, 5]\n",
      "x_img_4 | shape: [128, 64, 1, 1]\n",
      "x_img_5 | shape: [128, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "output, intermediate_rep = discrim_test(images.reshape((128, 3, 68, 68)))\n",
    "print(f\"{'output':6} | shape: {list(output.shape)}\")\n",
    "for i, tensor in enumerate(intermediate_rep):\n",
    "    print(f\"x_img_{i+1} | shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e823cf",
   "metadata": {},
   "source": [
    "### Variational Inference class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d17ddb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta=1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        #log_px = reduce(px.log_prob(x))\n",
    "        xhat = px.rsample()\n",
    "        log_px = - reduce(mse_loss(xhat, x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl # <- your code here\n",
    "        beta_elbo = log_px - self.beta * kl # <- your code here\n",
    "        \n",
    "        # loss\n",
    "        loss = - beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, xhat, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "925ac8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xhat   | shape: [128, 13872]\n",
      "loss   | mean =   2001.107, shape: []\n",
      "elbo   | mean =  -2001.107, shape: [128]\n",
      "log_px | mean =  -1999.831, shape: [128]\n",
      "kl     | mean =      1.276, shape: [128]\n"
     ]
    }
   ],
   "source": [
    "vi_test = VariationalInference(beta=1)\n",
    "loss, xhat, diagnostics, outputs = vi_test(vae, images)\n",
    "print(f\"{'xhat':6} | shape: {list(xhat.shape)}\")\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4cff918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 256\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# Discriminator\n",
    "stride = 1\n",
    "kernel_size = 5\n",
    "conv_channels = 32\n",
    "input_size = (batch_size//2, 3, 68, 68)\n",
    "discrim = Discriminator(input_size, \n",
    "                        conv_channels=conv_channels, \n",
    "                        kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "discriminator_optim = torch.optim.SGD(discrim.parameters(), 1e-2, momentum=0.9)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bbba7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved!\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [185]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m discrim\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Go through each batch in the training dataset using the loader\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Note that y is not necessarily known as it is here\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m training_loader:\n\u001b[1;32m     39\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m     tmp_batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mSingleCellDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     15\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass2index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoa\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     16\u001b[0m subfolder \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.*)_\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/numpy/lib/npyio.py:397\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    395\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    396\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 397\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "num_epochs = 5\n",
    "nll_val = []\n",
    "best_nll = 1000000.\n",
    "patience = 0\n",
    "max_patience = 20\n",
    "slope  = 2500.0\n",
    "discriminator_layers = 5\n",
    "loss_repr_func = nn.MSELoss(reduction='mean')\n",
    "bce_loss = nn.BCELoss(reduction='mean')\n",
    "discriminator_loss = []\n",
    "\n",
    "name = 'vae_plus'\n",
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "discrim = discrim.to(device)\n",
    "\n",
    "step = 0\n",
    "\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    \n",
    "    epoch += 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    batch_discrim_loss, batch_vae_loss = [], []\n",
    "    vae.train()\n",
    "    discrim.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in training_loader:\n",
    "        step += 1\n",
    "        \n",
    "        tmp_batch_size = x.size(0)\n",
    "        b1, b2 = torch.split(x, split_size_or_sections=tmp_batch_size//2)\n",
    "        batch_size_half = b1.size(0)\n",
    "        \n",
    "        b1 = b1.to(device)\n",
    "        b2 = b2.to(device)\n",
    "        \n",
    "        # Reshape to in order to be used as input to conv layers\n",
    "        b1_reshaped = b1.reshape(batch_size_half, 3, 68, 68).to(device)\n",
    "        b2_reshaped = b2.reshape(batch_size_half, 3, 68, 68).to(device)\n",
    "        \n",
    "        # True data is given label 1, while fake data is given label 0\n",
    "        true_label = torch.ones(batch_size_half, 1).to(device)\n",
    "        fake_label = torch.zeros(batch_size_half, 1).to(device)\n",
    "        \n",
    "        discrim.zero_grad()\n",
    "        vae.zero_grad()\n",
    "        \n",
    "        ### Step 1. Pass real images from batch 1 (b1) through VAE\n",
    "        loss_elbo, xhat, diagnostics, outputs = vi(vae, b1)\n",
    "        xhat_reshaped = xhat.reshape(batch_size_half, 3, 68, 68).to(device)\n",
    "        \n",
    "        ### Step 2. Pass x-hat (reconstructions) through Discriminator\n",
    "        # without calculating gradients\n",
    "        with torch.no_grad():\n",
    "            output_hat, inter_repr_fake = discrim(xhat_reshaped.detach())\n",
    "            print(output_hat.requires_grad)\n",
    "        \n",
    "        ### Step 3. Pass real images from batch 1 (b1) through Discriminator\n",
    "        # without calculating gradients\n",
    "        with torch.no_grad():\n",
    "            output_real_b1, inter_repr_real_b1 = discrim(b1_reshaped.detach())\n",
    "            print(output_real_b1.requires_grad)\n",
    "        \n",
    "        ### Step 4. Calculate the loss between the representations\n",
    "        # of the real images and the reconstructions \n",
    "        loss_repr = 0\n",
    "        loss_repr_list = []\n",
    "\n",
    "        def schedule_weight(delay):\n",
    "            \"\"\" Defines a delayed, linear, saturated schedulling function.\n",
    "            \"\"\"\n",
    "            step_norm = max(0.0, step - delay)\n",
    "            w = step_norm / slope\n",
    "            w = max(0.0, min(1.0, w)) #-- Bounded weight\n",
    "            return w\n",
    "\n",
    "        delays = [slope * (k+1) for k in range(discriminator_layers)]\n",
    "\n",
    "        for i, (repr_fake, repr_real) in enumerate(zip(inter_repr_fake, inter_repr_real_b1)):\n",
    "            loss_batch = loss_repr_func(repr_fake, repr_real)\n",
    "            loss_repr_list.append(loss_batch)\n",
    "\n",
    "            loss_weight = schedule_weight(delays[i])\n",
    "            loss_repr += loss_weight * loss_batch #-- Schedule-based weighted average\n",
    "\n",
    "        loss_total = loss_elbo + loss_repr\n",
    "        \n",
    "        ### Step 5. Backpropagate the gradients for the VAE\n",
    "        vae_optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        vae_optimizer.step()\n",
    "        \n",
    "        ### Step 6. Pass reconstrcutions (fake) and b2 real images through the discriminator,\n",
    "        # backpropagate the errors and update the weights of the discriminator\n",
    "        \n",
    "        # Get the output from the discriminator\n",
    "        output_hat, inter_repr_fake = discrim(xhat_reshaped.detach())\n",
    "        output_real_b2, inter_repr_real_b2 = discrim(b2_reshaped)\n",
    "        \n",
    "        # Get the output from the discriminator\n",
    "        output_hat, inter_repr_fake = discrim(xhat_reshaped.detach())\n",
    "        output_real_b2, inter_repr_real_b2 = discrim(b2_reshaped)\n",
    "\n",
    "        # Calculate the error and backpropagate \n",
    "        # For the reconstructions:\n",
    "        output_hat = output_hat.view(batch_size_half, 1)\n",
    "        output_hat_sigmoid = torch.sigmoid(output_hat)\n",
    "        error_fake = bce_loss(output_hat_sigmoid, fake_label)\n",
    "        error_fake.backward()\n",
    "\n",
    "        # And for the real images (b2)\n",
    "        output_real_b2 = output_real_b2.view(batch_size_half, 1)\n",
    "        output_real_b2_sigmoid = torch.sigmoid(output_real_b2)\n",
    "        error_true = bce_loss(output_real_b2_sigmoid, true_label)\n",
    "        error_true.backward()\n",
    "\n",
    "        # Update weights\n",
    "        discriminator_optim.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "        batch_discrim_loss.append((error_true/(error_true + error_fake)).item())        \n",
    "            \n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "        \n",
    "    discriminator_loss.append(np.mean(batch_discrim_loss))\n",
    "        \n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the validation loader\n",
    "        x, y = next(iter(val_loader))\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss_val, xhat, diagnostics, outputs = vi(vae, x)\n",
    "        print(loss_val)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    make_vae_plots(vae, x, y, outputs, training_data, validation_data)\n",
    "\n",
    "    if epoch == 1:\n",
    "            print('saved!')\n",
    "            torch.save(vae, result_dir + name + '_new.model')\n",
    "            best_nll = loss_val\n",
    "    else:\n",
    "        if loss_val < best_nll:\n",
    "            print('saved!')\n",
    "            torch.save(vae, result_dir + name + '.model')\n",
    "            best_nll = loss_val\n",
    "            patience = 0\n",
    "\n",
    "            samples_generated(result_dir + name, val_loader, extra_name=\"_epoch_\" + str(epoch))\n",
    "        else:\n",
    "            patience = patience + 1\n",
    "        \n",
    "    if patience > max_patience:\n",
    "        print(\"Max patience reached! Performing early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_vae_plots(vae, x, y, outputs, training_data, validation_data, save=False, \n",
    "               save_img=\"vae_out_beta_1_normal_epochs_5.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
