{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a36911f",
   "metadata": {},
   "source": [
    "## Implementation of standard VAE and trainning procedure\n",
    "\n",
    "The implementation inspired by: https://github.com/jmtomczak/intro_dgm/blob/main/vaes/vae_example.ipynb and https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/7_Unsupervised/7.2-EXE-variational-autoencoder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4f960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d83f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092bd35",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contain 68x68 images of single cells treated with different compounds. For each of the utilized compounds there is an associated mechanism of action (moa), which describes how the compound it affecting the cell. There are 12 different classes of moa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132ddce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.read_csv wiht pyarrow took 2185.364300966263 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "metadata = pd.read_csv('../data/metadata.csv', engine=\"pyarrow\")\n",
    "print(\"pd.read_csv wiht pyarrow took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cfd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DMSO_indx = metadata.index[metadata['moa'] == 'DMSO']\n",
    "DMSO_drop_indices = np.random.choice(DMSO_indx, size=260360, replace=False)\n",
    "\n",
    "metadata_subsampled = metadata.drop(DMSO_drop_indices).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df7b2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moa</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Microtubule stabilizers</td>\n",
       "      <td>89157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aurora kinase inhibitors</td>\n",
       "      <td>16810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DNA damage</td>\n",
       "      <td>16582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMSO</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microtubule destabilizers</td>\n",
       "      <td>15178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Epithelial</td>\n",
       "      <td>14955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eg5 inhibitors</td>\n",
       "      <td>12525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kinase inhibitors</td>\n",
       "      <td>11622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Protein synthesis</td>\n",
       "      <td>9715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>7491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Protein degradation</td>\n",
       "      <td>6589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DNA replication</td>\n",
       "      <td>5976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholesterol-lowering</td>\n",
       "      <td>5436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          moa  counts\n",
       "10    Microtubule stabilizers   89157\n",
       "1    Aurora kinase inhibitors   16810\n",
       "4                  DNA damage   16582\n",
       "3                        DMSO   16000\n",
       "9   Microtubule destabilizers   15178\n",
       "7                  Epithelial   14955\n",
       "6              Eg5 inhibitors   12525\n",
       "8           Kinase inhibitors   11622\n",
       "12          Protein synthesis    9715\n",
       "0            Actin disruptors    7491\n",
       "11        Protein degradation    6589\n",
       "5             DNA replication    5976\n",
       "2        Cholesterol-lowering    5436"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_subsampled.groupby(\"moa\").size().reset_index(name='counts').sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899fb8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DMSO': 0,\n",
       " 'Microtubule stabilizers': 1,\n",
       " 'Eg5 inhibitors': 2,\n",
       " 'Epithelial': 3,\n",
       " 'Actin disruptors': 4,\n",
       " 'Microtubule destabilizers': 5,\n",
       " 'Aurora kinase inhibitors': 6,\n",
       " 'Protein degradation': 7,\n",
       " 'DNA replication': 8,\n",
       " 'DNA damage': 9,\n",
       " 'Protein synthesis': 10,\n",
       " 'Kinase inhibitors': 11,\n",
       " 'Cholesterol-lowering': 12}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map from class name to class index\n",
    "classes = {index: name for name, index in enumerate(metadata[\"moa\"].unique())}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7ae2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, images_folder, class_map, transform = None):\n",
    "        self.df = annotation_file\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.class2index = class_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.loc[index, \"Single_Cell_Image_Name\"]\n",
    "        label = self.class2index[self.df.loc[index, \"moa\"]]\n",
    "        subfolder = re.search(\"(.*)_\", filename).group(1)\n",
    "        image = np.load(os.path.join(self.images_folder, subfolder, filename))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image.astype(\"int16\"))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b9315",
   "metadata": {},
   "source": [
    "### Code for the standard Variational Autoencoder (VAE)\n",
    "\n",
    "#### Functions for probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff01f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.0e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=12, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1.0 - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "    \n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2.0 * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "\n",
    "def log_normal_standard(x, reduction=None, dim=None):\n",
    "    D = x.shape[1]\n",
    "    log_p = -0.5 * D * torch.log(2.0 * PI) - 0.5 * x**2\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb9976",
   "metadata": {},
   "source": [
    "#### Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53e285ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # The init of the encoder network\n",
    "        self.encoder = encoder_net\n",
    "    \n",
    "    # The reparameterization trick for Gaussians\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        \n",
    "        # First, we need to get std from log-variance\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        \n",
    "        # Second, we sample epsilon from Normal(0, 1)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Finally, z is calculated\n",
    "        z = mu + std * eps\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    # This function implements the output of the encoder network \n",
    "    # (i.e., parameters of a Gaussian)\n",
    "    def encode(self, x):\n",
    "        # First, we calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Second, we must divide the output to the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "    \n",
    "    # Sampling procedure.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        # If we do not provide a mean an a log-variance, we must first calculate it:\n",
    "        if (mu_e is None) and (log_var_e is None):\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Or the final sample\n",
    "        else:\n",
    "            # Otherwise, we can simply apply the reparameterization trick!\n",
    "            if (mu_e is None) and (log_var_e is None):\n",
    "                raise ValueError('mu and log-var cannot be None!')\n",
    "        \n",
    "        z = self.reparameterization(mu_e, log_var_e)\n",
    "        return z\n",
    "    \n",
    "    # This function calculates the log-probability that is later used for calculating the ELBO.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        # If we provide x alone, then we can calculate a corresponding sample.\n",
    "        if x is not None:\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        else:\n",
    "        # Otherwise, we should provide mu, log-var and z!\n",
    "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
    "                raise ValueError('mu, log-var and z cannot be None!')\n",
    "                \n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "    \n",
    "    # PyTorch forward pass: it is either log-probability (by default) or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83876de4",
   "metadata": {},
   "source": [
    "#### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3559a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # The decoder network.\n",
    "        self.decoder = decoder_net\n",
    "        # The distribution used for the decoder (it is categorical by default)\n",
    "        self.distribution = distribution\n",
    "        # The number of possible values. This is important for the categorical distribution.\n",
    "        self.num_vals = num_vals\n",
    "        \n",
    "    def decode(self, z):\n",
    "        h_d = self.decoder(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            b = h_d.shape[0]\n",
    "            d = h_d.shape[1]\n",
    "            h_d = h_d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
